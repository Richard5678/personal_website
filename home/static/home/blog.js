function changeLangBlog(blog, type) {
    changeLangNav(type);
    changeLangBlogSec(type);

    var title = document.getElementById('blog_title');
    console.log(blog)
    title.innerHTML = blog_content[blog][type]["title"];

    var date = document.getElementsByClassName('blog_date');
    for (i = 0; i < date.length; i++) {
        date[i].innerHTML = blog_content[blog]["date"];
    }

    var intro = document.getElementsByClassName('blog_intro');
    for (i = 0; i < intro.length; i++) {
        intro[i].innerHTML = blog_content[blog][type]["intro"];
    }

    var motivation = document.getElementsByClassName('blog_motivation');
    console.log(blog);
    for (i = 0; i < motivation.length; i++) {
        motivation[i].innerHTML = blog_content[blog][type]["motivation"][i];
    }

    var body = document.getElementsByClassName('blog_body');
    for (i = 0; i < body.length; i++) {
        body[i].innerHTML = blog_content[blog][type]["body"][i];
    }
}

var blog_content = {
    first: {
        "date": "11/01/2021",
        "en": {
            "title": "First Post",
            "intro": "Welcome to my first post! I just want to use the space below to talk about some of the motivations of writing blogs as well as some types of blogs I will be writing about.",
            "motivation": ["See below:"],
            "body": ["While I was setting up my personal website this past week, I had the idea of having a"
                + "section that shares some of personal experiences and understandings on the many issues that we face today."
                + "I think it is a great way to give people who are viewing this website an idea of what my interests are and more importantly where do I spend time on."
                + "As an initial thought, I will be mainly writing on three different topics:"
                + "<ol>"
                + "<li>Travelling Experiences</li>"
                + "   <li>Book Reports</li>"
                + "    <li>Coding Related Topics (Algorithms, Data Structores, etc)</li>"
                + "</ol>"
                + "<span class='paragraph'> "
                + "</span>"
                + "Another moviation for writing blogs is to help myself organize some of my thoughts."
                + "Also, I plan to add support for both Chinese and English which means I get the chance to practice writing in Chinese which I haven't done so for a long time."
                + "<span class='paragraph'></span>"
                + "<h3 class='margin_top'>Special Thanks</h3>"
                + "<hr>"
                + "<p>I want to use this sectioin to thank everything that has helped me build this website.</p>"
                + "<p>Most importantly, I want to thank <a href='https://www.youtube.com/watch?v=Nn7EX3zkGUo&list=PLhQjrBD2T380xvFSUmToMMzERZ3qB5Ueu'>CS50's Web Programming with Python and Javascript.</a>"
                + "Instructor Brian Yu is a truly amazing teacher and most of the techniques and concepts I used in developing this website come from this course.</p>"
                + "<p>I also want to thank those who try to answer questions to their best of abilities on stackoverflow. We don't truly appreciate you until we've faced problems that take days to fix.</p>"
                
                + "<p style='padding-top: 30px; font-size: 25px;'>Thanks for reading! Enjoy!</p>"]
        },
        "zh": {
            "title": "第一个博客",
            "intro": "欢迎来到我的第一个博客。我想用以下的空间来谈谈我写博客的动机以及所写博客的种类。",
            "motivation": ["见以下:"],

            "body": ["这周我在编写个人网站时有了加一个“博客”分区的想法。我想用它来分享我的一些个人经历以及对我们每天面对问题的理解。"
                + "我觉得这是一个很好的方式来分享我的兴趣是什么以及更重要的是我是怎样分配我的时间的。"
                + "作为一个最初的想法，我准备主要写关于以下三个主体的博客："
                + "<ol>"
                + "     <li>旅行经历</li>"
                + "     <li>读书报告</li>"
                + "     <li>编程相关的主体（算法，数据结构...)</li>"
                + "</ol>"
                + "<span class='paragraph'> "
                + "</span>"
                + "写博客的另一个动机是帮助我自己组织思绪。"
                + "除此之外，我准备同时支持中文和英文，这意味着我有机会练习用中文写作。"
                + "<span class='paragraph'></span>"
                + "<h1 class='margin_top'>特别感谢</h1>"
                + "<hr>"
                + "<p>我想用这个区域来感谢所有帮助我开发这个网站的东西。</p>"
                + "<p>首要的是我想感谢 <a href='https://www.youtube.com/watch?v=Nn7EX3zkGUo&list=PLhQjrBD2T380xvFSUmToMMzERZ3qB5Ueu'>CS50's Web Programming with Python and Javascript.</a>"
                + "讲师 Brian Yu 是一个真正出色的老师。绝大多数我在发开这个网站中用到的概念和技术都是来自这个课程。</p>"
                + "<p>我还想感谢所有尽全力在stack overflow 上面回答问题的人。我们不会真正感激你们直到遇到需要几天来解决的问题。</p>"
                
                + "<p style='padding-top: 30px; font-size: 25px;'>谢谢阅读！</p>"]
        }
    },
    "gradDescent": {
        "date": "11/04/2021",
        "en": {
            "title": "Gradient Descent in Machine Learning",
            "intro": "This article covers the implementation and theoretical framework of gradient descent. Topics such as feature scaling and cross-validation are also included to improve testing and training."
                    + "Most the theoritical work presented below is based on <a href=\"https://github.com/maxim5/cs229-2018-autumn/blob/main/notes/cs229-notes1.pdf\">Prof. Andrew Ng's notes </a>.",
            "motivation": ["I have been learning about machine learning recently. For simple models like linear regression and logistic regression"
                            + "it is not hard to get an idea of what they mean theoritically."
                            + "However, as I proceded into implementing them with real-world dataset, there were many obstales."
                            + "So I want to use this blog to talk about what problems I faced and how I solved them."],
            "body": ["<h3>Theoritical Framework</h3>"
                    + "<hr>"
                    + "<p>The basic idea of gradient descent is to update the parameter &Theta; toward the steepest direction of cost function J(&Theta;).</p>"
                    + "Its update rule is given by:"
                    + "<div style=\"text-align:center;\">"
                    + "     <img src=\"/static/home/grad1.png\" style=\" width: 70%; margin: 20px\">"
                    + "</div>"
                    + "where &alpha; is the learning rate which determine how big a step each update takes."
                    + "<p style=\"margin-top: 20px;\"> The loss function J(&Theta;) measures how good our model fits the dataset. A common choice for the loss function the mean squraed error (mse) which is given by: </p>"
                    + "<div style=\"text-align:center;\">"
                    + "     <img src=\"/static/home/grad2.png\" style=\" width: 70%; margin: 20px\">"
                    + "</div>"
                    + "where the additional 2 on the denominator is just there to make the derivation easier later. Now our task becomes minizing J(&Theta;)."
                    + "<p>Taking the partial derivative of J(&Theta;) gives:</p>"
                    + "<div style=\"text-align:center;\">"
                    + "     <img src=\"/static/home/grad3.png\" style=\" width: 70%; margin: 20px\">"
                    + "</div>"
                    + "Hence our update rule becomes:"
                    + "<div style=\"text-align:center;\">"
                    + "     <img src=\"/static/home/grad4.png\" style=\" width: 70%; margin: 20px\">"
                    + "</div>"
                    + "<p style=\"margin-top: 30px\"> The update rule we've obtained above updates entries in $Theta; one at a time which makes it look messy in the implementaion."
                    + "Let us see how we can perform the update rule simultaneously for all entries of &Theta; using matrix multiplication. We start off by defining the design matrix X to be: </p>"
                    + "<div style=\"text-align:center;\">"
                    + "     <img src=\"/static/home/grad5.png\" style=\" width: 70%; margin: 20px\">"
                    + "</div>"
                    + "where each row is a training example."
                    + "<p style=\"margin-top: 30px\">Observe that:</p>"
                    + "<div style=\"text-align:center;\">"
                    + "     <img src=\"/static/home/grad6.png\" style=\" width: 70%; margin: 20px\">"
                    + "</div>"
                    + "Hence we've completed all the theoritical background needed to implement the gradient descent algorithm"
                    + "<h3 style=\"margin-top: 40px\">Implementation</h3>"
                    + "<hr>"
                    + "Here is an implementation of gradient descent for linear regression:"
                    + "<div style=\"text-align:center;\">"
                    + "     <img src=\"/static/home/grad7.png\" style=\" width: 70%; margin: 20px\">"
                    + "</div>"
                    + "Note that the algorithm stops if the number of iterations sepecified by n_epoch is completed or the updated &Theta; is within 0.00001 of the original &Theta;."
                    + "<p>Having this algorithm at hand, one other thing we need to do is make sure matrix X is properly scaled.</p>"
                    + "<p>By that I mean each element in each column of X is updated as </p>"
                    + "<div style=\"text-align:center;\">"
                    + "     <img src=\"/static/home/grad8.png\" style=\" width: 50%; height = 50%; margin: 20px\">"
                    + "</div>"
                    + "where x hat is the mean of the column and sigma is the standard deviation of the column."
                    + "<p>By scaling the features, we have effectively converted all the elements of X to relatively close to 0.</p>"
                    + "Many would then ask why do we need feature scaling. Well, the update rule for gradient descent dependents on x<sub>j</sub> which is a value in the matrix."
                    + "In most occations, the values in each column of X varies dramatically. "
                    + "For example, a column may contain the year of a transaction which is typically greater than 2000. Another columns may contain a binary data (0 or 1) indicating if a house has been selled before."
                    + "Such big difference bewteen columns of the data will make parameter &Theta; converge faster on some entries than others (faster on columns in which its data are big to be specific)."
                    + "In fact, when I was implementing this algorithm for the real estate pricing data. It seems like &Theat; will never converge on columns with small values."
                    + "</p>"
                    + "Therefore, it is essential to do feature scaling before performing gradient descent, espicially when the value of data varies greatly between columns of X."
                    + "<p>For convience, you can use the sklearn library to perform feature scaling as follows</p>"
                    + "<div style=\"text-align:center;\">"
                    + "     <img src=\"/static/home/grad9.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                    + "</div>"
                    + "<h3 style=\"margin-top:30px\">Cross Validation</h3>"
                    + "<hr>"
                    + "After implementating gradient descent to fit a model, the next thing we need to do is test how good the model is."
                    + "A common method for testing ML models is called cross validation. The idea behind is training the model with a fraction of the data and test it with the data that was left out during training."
                    + "A more advanced way of using this is called k-fold cross validation which divides the dataset into k subsets. Then train the model with every (k - 1) subsets and test it with the 1 subset that was left out."
                    + "Finally, take the average of those testing results. Here is an implementation:"
                    + "<div style=\"text-align:center;\">"
                    + "     <img src=\"/static/home/grad10.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                    + "</div>"
                    + "<h3 style=\"margin-top:30px\">Projects</h3>"
                    + "<hr>"
                    + "To give a broader overview of the real-world application of gradient descent, I have implemented two projects - one for <a href=\"https://github.com/Richard5678/Machine-Learning/blob/main/housing%20-%20linReg.ipynb\">housing price predictor using linear regression</a>"
                    + " and one fore <a href=\"https://github.com/Richard5678/Machine-Learning/blob/main/heart%20problem%20-%20logReg.ipynb\">heart disease predictor using logisitic regression</a>. "
                    + "I have also used various methods including sklearn models and the normal equation to test the output of my gradient descent implementation. As you can see on github, we get the same output."]
        },
        "zh": {
            "title": "机器学习中的梯度下降",
            "intro": "这篇博客概述了梯度下降(Gradient Descent) 的理论框架及软件实现。文章还包含了特征缩放(Feature Scaling),  交叉验证(Cross Validation) 等主体还提升测试和训练效率。"
                    + "以下大部分理论背景来自于<a href=\"https://github.com/maxim5/cs229-2018-autumn/blob/main/notes/cs229-notes1.pdf\">Andrew Ng教授的笔记</a>",
            "motivation": ["最近我开始学习机器学习基础。对于像线性回归和罗辑回归这样的相对基础的模型， "
            + "它们的理论架构比较好理解。"
            + "然而当我用真实数据应用这些模型时, 遇到了许多困难。"
            + "所以我想用这个博客来谈谈我遇到的困难以及我是怎样解决它们的。"],

            "body": ["<h3>理论背景</h3>"
            + "<hr>"
            + "<p>梯度下降的基本原理源于让参数&Theta;朝成本函数J(&Theta;) 最陡峭的方向前进一步。</p>"
            + "它的更新公式是:"
            + "<div style=\"text-align:center;\">"
            + "     <img src=\"/static/home/grad1.png\" style=\" width: 70%; margin: 20px\">"
            + "</div>"
            + "&alpha; 是学习进度。它掌控每一步参数前进的大小。"
            + "<p style=\"margin-top: 20px;\"> 成本函数 J(&Theta;) 用来衡量模型对数据的贴合程度。一个常见的成本函数就是均方误差: </p>"
            + "<div style=\"text-align:center;\">"
            + "     <img src=\"/static/home/grad2.png\" style=\" width: 70%; margin: 20px\">"
            + "</div>"
            + "分母上多余的2是用来简化将来的微分运算的。现在我们的任务变成了求J(&Theta;)的最小值."
            + "<p>J(&Theta;) 的偏导数为:</p>"
            + "<div style=\"text-align:center;\">"
            + "     <img src=\"/static/home/grad3.png\" style=\" width: 70%; margin: 20px\">"
            + "</div>"
            + "进而我们的更新公式为:"
            + "<div style=\"text-align:center;\">"
            + "     <img src=\"/static/home/grad4.png\" style=\" width: 70%; margin: 20px\">"
            + "</div>"
            + "<p style=\"margin-top: 30px\"> 我们以上得到的更新公式每次只更新$Theta; 里的一个数字。这会使其在软件中的运算变得复杂。"
            + "所以让我们来看看怎样运用矩阵乘法来同时更新&Theta; 里的每个数字。我们从定义矩阵X开始: </p>"
            + "<div style=\"text-align:center;\">"
            + "     <img src=\"/static/home/grad5.png\" style=\" width: 70%; margin: 20px\">"
            + "</div>"
            + "每行为一个训练例子。"
            + "<p style=\"margin-top: 30px\">观察发现:</p>"
            + "<div style=\"text-align:center;\">"
            + "     <img src=\"/static/home/grad6.png\" style=\" width: 70%; margin: 20px\">"
            + "</div>"
            + "到此，我们已经概述了所有用软件实现梯度下降的理论背景。"
            + "<h3 style=\"margin-top: 40px\">软件实现</h3>"
            + "<hr>"
            + "以下是个在线性回归中运用梯度下降的算法:"
            + "<div style=\"text-align:center;\">"
            + "     <img src=\"/static/home/grad7.png\" style=\" width: 70%; margin: 20px\">"
            + "</div>"
            + "要注意的是这个算法会在训练次数（n_epoche) 完成或更新后的&Theta; 和更新前距离小于0.00001是停止。"
            + "<p>有了这个算法，一个我们要注意的东西就是特征的缩放。</p>"
            + "<p>其意思为每个矩阵X中的数字都会根据其所在的列而更新为: </p>"
            + "<div style=\"text-align:center;\">"
            + "     <img src=\"/static/home/grad8.png\" style=\" width: 50%; height = 50%; margin: 20px\">"
            + "</div>"
            + "x 横线为这个数字所在列的平均数，sigma为其所在列的标准差。"
            + "<p>通过特征缩放，我们实际上把矩阵X中的所有数字都变为了在0 左右的数字。</p>"
            + "很多人会问我们为什么要缩放特征(训练例子)呢。以上我们能看到梯度下降依赖于 x<sub>j</sub>（x<sub>j</sub>是矩阵X中的一个数字）。"
            + "在大多数情况下, 矩阵X的列与列里数值的大小有很大差别。 "
            + "比如，一列可能是所有交易的年份（大多大于2000）， 而另一列为二进制数字（0，1)用来表示这个房子之前有没有被卖过。"
            + "列与列之间巨大的数数值区别导致在梯度下降的过程中一个列所对应的参数会更快的聚拢。(确切的说，数值更大的列会更快聚拢)。"
            + "事实上，当我在为房屋交易数据编写这个算法时，有些列对应的参数看起来似乎永远不会聚拢到最低点。"
            + "</p>"
            + "所以，在使用梯度下降算法前，特征（训练例子）的缩放十分关键 (特别是在矩阵X列与列见数值差距大时)。"
            + "<p>为了方便起见，可以用sklearn library 来按照以下方法进行特征缩放:</p>"
            + "<div style=\"text-align:center;\">"
            + "     <img src=\"/static/home/grad9.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
            + "</div>"
            + "<h3 style=\"margin-top:30px\">交叉验证</h3>"
            + "<hr>"
            + "在用梯度下降来训练一个模型后, 接下来我们要做的就是来测试我们的模型够不够好。"
            + "一个常用的手段就是交叉验证。其背后的想法就是用一部分数据来训练模型，之后用剩下的数据（训练中没用到）的数据来测试模型。"
            + "一个更高级的方法较多k重交叉验证。其想法就是将数据分为 k 子集。 之后用每个(k-1)子集的组合来训练模型之后用剩下的一个子集来测试模型。"
            + "最后求出它们的平均数来作为衡量模型好坏的标准。这里是个程序案例:"
            + "<div style=\"text-align:center;\">"
            + "     <img src=\"/static/home/grad10.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
            + "</div>"
            + "<h3 style=\"margin-top:30px\">项目</h3>"
            + "<hr>"
            + "为了来更全面的展现梯度下降实际的应用，我做了两个小项目 - 一个是 <a href=\"https://github.com/Richard5678/Machine-Learning/blob/main/housing%20-%20linReg.ipynb\">房屋价格预测器（线性回归）</a>"
            + " 另一个是 <a href=\"https://github.com/Richard5678/Machine-Learning/blob/main/heart%20problem%20-%20logReg.ipynb\">心脏病预测器（逻辑回归）</a>. "
            + "我同时还用了像sklearn 模型 和 正规方程(normal equation) 等方法来测试我的梯度下降算法的输出。正如在github上展示的那样，它们的输出是一样的。"]
        }
    },
    "randomForest": {
        "date": "12/07/2021",
        "en": {
            "title": "Implementing Random Forest from scratch in Python",
            "intro": "This article is a walk-through of an implementation of random forest classifier without using any implementations from existing libraries.",
            "motivation": ["I am recently studying different kinds of machine learining algorithms. The most effective way of learning different kinds of algorithms is actually implement them concretely."
                            + "With is in mind, I have previously implemented linear and logistic regressions which are relatively simple modles."
                            + "My next target is some type of decision tree algorithms. After some research, it has became apprant that random forest and gradiented boosting trees are the two most popular and effective tree based algorithms."
                            + "Unlike many other types of algorithms like convolutional neural networks, there is actually really limited amount of information about how the two algorithms above are implemented using \"easy of understand\" code."
                            + "So after implenting my own version of random forest following weeks of struggling, I want to share my implementation and the lessons I have learned."],
            "body": ["<h3 class='margin_top'>What is Random Forest?</h3>"
                + "The simple answer is: Random Forest is a type of decision tree that uses bagging (train multiple trees). "
                + "A decision tree simply asks a question at each node and split the data at the current node into two children nodes (one with all examples that responds yes to the splitting question and the other responds no)."
                + "The next question would be how do we select such split. Well, the intuitive goal of a split is to groups examples with similar labels together."
                + "For example, in a regression problem, we want to groups similar target values together. Similarly, we want to groups same labels together in classification tasks."
                + "So we will need to define some type of loss function and quantifies how good is a split."
                + "A typical choice for regression tasks is MSE where the predicted value is the mean of training examples at that node."
                + "For classification tasks, a common choice of loss function is the Gini index which is what we are going to use in the example below."
                + "A Random Forest consists of multiple decision trees where each is trained on a subset of features as well as a subset of training examples. "
                + "At test time, a Random Forest algorithm will take the majority class predicted by all trees in classification tasks."
                + "<h3 class='margin_top'>Implementation</h3>"
                + "Let us start off by formalizing the steps a Random Forest classifier will take to train a single tree:"
                + "<ol style=\"margin-left:10%\">"
                + "     <li>Randomly sample features and training examples.</li>"
                + "     <li>Find optimal split by calculating the loss for each possible value of the given feature.</li>"
                + "     <li>Split the data at the current node based on the optimal split point found above.</li>"
                + "     <li>Recursively build up the left and right sub-tree until the max-depth has been reached</li>"
                + "</ol>"
                + "<p style=\"margin-top:30px\">The core of the program is the function random_forest given below:</p>"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random1.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "What this function does is calling build_tree function \"treeLength\" number of times and update the progress bar accordingly. "
                + "Then the tree returned from build_tree function will be stored in a list named decision_trees which is returned to the client. "
                + "<p style=\"margin-top:30px\">Let's take a look at the function build_tree now:</p>"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random2.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "Since we are recursively building the tree up, there needs to be a base case at the start of the function. "
                + "In this case, the base case is activated when we have reached the max-depth or there is zero features. "
                + "After that, the entire function can be divided into 4 main sections:"
                + "<ol style=\"margin-left:10%\">"
                + "     <li>find the best split: line 41 - 49</li>"
                + "     <li>construct node and fill in attributes: line 52 - 55</li>"
                + "     <li>build up left and right sub-tree under certain conditions: line 59 - 62</li>"
                + "     <li>find the label of the node at leaf node: line 65 - 73</li>"    
                + "</ol>"
                + "We start off by sampling &#8730;(total number of features) features. From line 41 to 49, we use each value of the sampled features as a decision boundary. "
                + "The split function returns two lists, one with indices of examples less than the split point and the other with examples greater than or equal to the split point:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random3.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "Then we call the get_gini functions which returns the weighted gini index of the sub groups splitted above as well as a list containing the gini index of both groups:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random4.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "Note that the gini of a set of data is give by:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random5.png\" style=\" width: 20%; height = 50%; margin: 20px\">"
                + "</div>"
                + "where i is each label in set and p is the fraction of example with label i. For example if we have a set of 4 examples (2 positive and 2 negative), the gini index is give by 1 - (2/4)<sup>2</sup> - (2/4)<sup>2</sup> = 0.5. "
                + "Note that we are using gini index as the loss function for splitting and the less the gini index the better the split. But how does gini index make sense? Suppose we have two labels so that we plot the gini index:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random6.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "We can see the gini index is the highest when x = 0.5 which means the set of data contains half of positive labels and half negative labels. The way decision tree classify data is by classifying all examples in the node by the majority label in that node. "
                + "So 50/50 the worst scenerio because we will classify half of the examples wrong. On the other hand 0/100 or 100/0 is the best scenetio since we will classify all examples correctly and they indeed have the lowest loss (gini index). "
                + "Intuitively, we want the node to have the highest purity and it is not hard to generalize this idea to multi-label classification scenerios. <br>"
                + "You may have noticed that the formula get_gini function uses for gini index is a bit different than the one given above. This is because of the following algebric manipulation:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random7.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "where S is the amount of data in the group specified by its subscript."
                + "<p style=\"margin-top:50px;\">After finding the optimal split point, we initialize the node and store the split point in it (line 52 - 55). </p>"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random2.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "Then we need to decide if we want to extend the tree by building up left and right sub-tree (line 59 - 62). We would only do so if both right and left sub-groups have positive lengths "
                + "and the weighted gini index of the split is less than the gini index of the current node." 
                + "Intuitively, if a sub-group contains 0 examples, then there is no point to build up the sub-tree as it will continue to find the same split to and pass 0 examples to one children and pass all examples to the other children until reaching the max-depth. "
                + "The second condition on gini index makes sure the split does indeed improving the decision tree which makes sense as we are trying to make the classifier better. <br>"
                + "<p style=\"margin-top:30px;\"></p>"
                + "The last thing we need to do find the label of the node at leaf node (line 65-73). We simply find the label with the most training examples at that node and store it in the label attribute in Node. "
                + "<span style=\"margin-top: 30px;\"></span> To make a prediction on x, we simply pass it along with the root of a tree to the function classify:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random8.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "We will do this to all the trees returned by random_forest and take the mode of all predictions as the final prediction. To see how to do this concretely, the function eval performs a n-fold cross validation on the dataset:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random9.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "Firestly, cross_val_split splits the data into n-fold equal number of sub-sets. Data of shape (m, n) will become (n_fold, m / n_fold, n): "
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random10.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "Variable set contain the all indices that have not yet been sampled. In each iteration we sample from set fold_size number of items and push them as a list into datasets. <br>"
                + "Line 40 - 42 prepares train_set and test-set. Then we pass each element in the train_set to the function predict which then calls classify to make predictions:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random11.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "Finally, we print the train, validation accuracy to the output as well as the mean validation accuracy at the end. "
                + "<h1 style=\"margin-top: 50px;\">Results</h1> <hr>"
                + "We are able to get 86+% accuracy on the validation set with 10-fold cross validation:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random12.png\" style=\" width: 90%; height = 50%; margin: 20px\">"
                + "</div>"
                + "<h1 style=\"margin-top: 50px;\">Parameter Tuning</h1> <hr>"
                + "In Random Forest, there are two hyperparameters that that we need to set: max-depth and the number of trees trained. " 
                + "To find out the best combination of hyper parameters we include a set of possible values in the numpy array n and depth. Then we test out all possible combiantions using the sklearn library: "
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random13.png\" style=\" width: 90%; height = 50%; margin: 20px\">"
                + "</div>"
                + "The main reason to use the sklearn library is the speed is offers. Although my implementation uses numpy whenever I can to take advantage of vectorization, it is a lot of slower than sklearn. "
                + "From the following screenshot we can see that 50 trees along with a max-depth of 6 achive the best validation accuracy (85.93%) which is actually lower than the one I got in my implementation (with the same hyperparameters). "
                + "From the first graph at the bottom we can see that as max-depth increase, training accuracy increases linearly to high 90s while the validation error increases at the first half but become horizontal in the second half. "
                + "This demonstrates the overfitting becomes more dramatic as max-depth gets larger. <br>"
                + "The second graph shows as the number of trees trained increases, both training and validation accuracy increases. "
                + "However, this trend becomes minimal after n goes above 25."
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random14.png\" style=\" width: 90%; height = 50%; margin: 20px\">"
                + "</div>"
                + "<h1 style=\"margin-top: 50px;\">Implementation</h1>"
                + "<hr>"
                + "See <a href=\"https://github.com/Richard5678/Machine-Learning/blob/main/heart%20problem%20-%20random%20forest.ipynb\">github</a> for details. "]
                
        },
        "zh": {
            "title": "用Python从零开始软件实现随机森林",
            "intro": "这篇文章展示了一个随机森林分类器的软件实现（没有运用任何已有的直接实现随机森林的函式库）。",
            "motivation": ["我最近在学习机器学习的算法。学习不同算法最有效的方法就是用软件把他们写出来。"
                            + "根据这个想法，我之前用软件实现了较简单的线性回归和罗辑回归。"
                            + "我的下一个目标变为了决策树的一种。在做了一些研究后，我发现随机森林和梯度提升树是其中最受欢迎且有效的已树为基础的算法。"
                            + "不同于像卷积神经网络的其他算法，网上很难能找到的关于它们的以理解的软件实现（不运用直接的函式库）。"
                            + "所以在挣扎了几周后编写完我自己版本的随机森林后，我想分享一下我的成果以及我所学到的东西。"],
            "body": ["<h3 class='margin_top'>什么是随机森林?</h3>"
                + "简单的答案是: 随机森林是一种使用了引导聚拢算法(训练多个树)的决策树。"
                + "一个决策树在每一个节点会问一个问题并且根据每个数据回答这个问题的记过而将其分流到两个子节点 (回答\"是\"的到一个节点，\"不是\"的到另一个节点。"
                + "那么下一个问题就是我们怎么决定在每一个节点问什么问题呢。直观上讲，我们想要在分流后把标签相近的训练例子聚到一起。"
                + "比如，在一个回归的问题中（连续性输出), 我们想要把标签数值相近的训练例子分流到一起。相似的是在分类问题中我们标签同样的训练例子分流到一起。"
                + "所以我们需要定义一个成本函数来测量每个分流的好坏。"
                + "在回归问题中一个常见的选择是均方差，其中预测的值为在这个节点所有训练例子的平均数。"
                + "在分类问题中一个常见的选择为基尼系数(gini index)。我们将会在以下实例中用到它。"
                + "一个随机森林包含多个决策树。其中每个都是用随机选取的一部分特征以及训练案例训练的。"
                + "在分类问题的测试阶段，随机森林会选取所有决策树预测结果的众数作为最后的预测。"
                + "<h3 class='margin_top'>软件实现</h3>"
                + "首先，我们正式给出随机森林训练一个决策树的步骤："
                + "<ol style=\"margin-left:10%\">"
                + "     <li>随机抽取特征及训练例子</li>"
                + "     <li>通过计算每个抽取特征的所有数值成本来找到最佳分界点</li>"
                + "     <li>将现在节点的训练例子根据以上找到的分界点分成两组</li>"
                + "     <li>递归的搭建子决策树直至到设定的最深深度</li>"
                + "</ol>"
                + "<p style=\"margin-top:30px\">整个算法的核心为以下的random_forest函数:</p>"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random1.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "这个函数会重复使用build_tree函数\"treeLength\"遍并更新进度条。"
                + "之后从build_tree返还的树会被保存在decision_trees里并在最后返回给用户（client)。"
                + "<p style=\"margin-top:30px\">以下为build_tree 函数:</p>"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random2.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "因为我们是运用递归来建造书的，所以我们需要一个终止条件（base case)。"
                + "在这个案例中终止条件会在已到最高深度或有0个特征时激活。"
                + "之后这个函数可以被分为4大部分："
                + "<ol style=\"margin-left:10%\">"
                + "     <li>找到最佳分界点: 41 - 49行</li>"
                + "     <li>建造节点并添加节点信息: 52 - 55行</li>"
                + "     <li>在一定情况下建造出左右子树: 59 - 62行</li>"
                + "     <li>在叶节点找到节点的标签: 65 - 73行</li>"    
                + "</ol>"
                + "首先我们随机抽取 &#8730;(特征总数量)的特征。从41到49行, 我们将每个被抽取的特征的每一个数值来作为分界点。"
                + "split函数返还两个list, 一个包含所有小于分界点的训练例子的索引，另一个包含所有大于等于分界点训练例子的索引:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random3.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "之后我们使用get_gini函数得到子群的加权基尼系数(gini index)平均数和每个子群的基尼系数:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random4.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "一组数据的基尼系数为:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random5.png\" style=\" width: 20%; height = 50%; margin: 20px\">"
                + "</div>"
                + "i是这组数据中每个可能的标签，p为标签i在这组数据中所占的比。加入我们有四个例子(两正两反)，那么其基尼系数为1 - (2/4)<sup>2</sup> - (2/4)<sup>2</sup> = 0.5。"
                + "需要注意的是我们用基尼系数作为每次分流的成本函数所以基尼系数越小，那么分流就越好。但是怎么来理解基尼系数呢？假设我们有两种标签那么我们可以得到基尼系数的图像:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random6.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "我们可以看到基尼系数在x = 0.5时最高。在这种情况下我们有一半的例子有正标签而另一半为反标签。决策树在预测时会把所有在这个叶节点的训练例子都预测为这个也节点中最多的标签。"
                + "这意味着50/50 是最差的结果因为我们会对一般的例子预测错误。相反0/100或100/0是最好的情况因为所有训练例子都会被正确的预测并且它们同时也有最低的成本函数值(基尼系数)。"
                + "直观上讲, 我们想要节点拥有最高的纯净度并且我们能将这个想法延展到多种标签的分类情况。 <br>"
                + "你可能注意到了我们在get_gini函数中所用的基尼系数公式与上面给出的不太一样。这是因为以下的代数运算:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random7.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "S为其下标所指群的例子数量。"
                + "<p style=\"margin-top:50px;\">在找到最佳分界点时, 我们初始化节点并存入分界点(52 - 55行).</p>"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random2.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "之后我们需要决定我们是否想要通过搭建所有子树来延展整个决策树(59 - 62行)。我们只在所有左右两个子群都有至少一个例子"
                + "并且它们的加权基尼系数平均数小于现在的节点的基尼系数时才继续大家子树。" 
                + "直观上，如果一个子群有0个训练例子，那么没有在继续搭建子树的必要因为其子数会找到一样的分界点并不停的向一侧延展决策树知道遇到最高深度。"
                + "第二个关于基尼系数条件是用来确保我们的决策树在分流后确实比分流前要更好。<br>"
                + "<p style=\"margin-top:30px;\"></p>"
                + "最后我们要做的是找到叶节点的标签(65-73行)。我们只需找到叶节点所有训练例子中例子最多的标签作为整个叶节点的标签并将其存在节点的label变量里。"
                + "<span style=\"margin-top: 30px;\"></span>当预测x时, 我们只需将其与树的根送到classify函数中:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random8.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "我们将对随机森林中所有的树做以上操作并在分类问题中取所有树的预测结果的众数作为随机森林的最后预测。我们可以通过以下的eval函数来看看如何通过n折交叉验证（n-fold cross-validation)来软件实现随机森林的预测:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random9.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "首先, cross_val_split函数将数据分为n-fold个等等大小的部分。(m, n)形状的数据会变为(n_fold, m / n_fold, n): "
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random10.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "set变量包含了所有没被抽到过的索引。在每轮循环中我们从set中抽取fold_size大小的数据并将他们存到datasets里。<br>"
                + "40 - 42行准备了train_set和test-set。之后我们将train_set每个训练例子送到predict函数，其将使用classify函数来给出最终的预测:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random11.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "最后，我们打出训练以及验证正确率并在最后打出平均验证正确率。"
                + "<h1 style=\"margin-top: 50px;\">结果</h1> <hr>"
                + "我们能够10折交叉验证的验证数据中得到86+%的平均正确率:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random12.png\" style=\" width: 90%; height = 50%; margin: 20px\">"
                + "</div>"
                + "<h1 style=\"margin-top: 50px;\">参数调试</h1> <hr>"
                + "在随机森林中我们有两个超参数需要设置：最高深度以及训练的决策树数量。" 
                + "为了找到最佳的超参数组合，我用了两个numpy array来存了一些可能的数值。之后我们用sklearn函式库来测试所有的组合: "
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random13.png\" style=\" width: 90%; height = 50%; margin: 20px\">"
                + "</div>"
                + "运用sklearn函式库的主要原因是因为其的速度。尽管我在我写的程序中尽可能的运用numpy来利用其向量化带来的速度提升，但是还是比sklearn慢很多。"
                + "在以下的输出中我们可能看到50个树以及最高深度6的组合得到了最高的验证正确率(85.93%)。这个正确率实际上比我的版本的到的还要低(使用了一样的超参数)。"
                + "在下面第一个图像中我们能看到当最高深度提升时训练正确率直线提升到97以上，但验证正确率在前半段上升过后变得平缓了。"
                + "这表明了在最高深度提高的过程中过拟合(overfitting)越来越剧烈。<br>"
                + "第二个图像展示出当训练树的数量提高时，训练和验证正确率同时提高。"
                + "但是这个趋势变得在大于25的树后变得十分微微小。"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random14.png\" style=\" width: 90%; height = 50%; margin: 20px\">"
                + "</div>"
                + "<h1 style=\"margin-top: 50px;\">软件实现</h1>"
                + "<hr>"
                + "详情见<a href=\"https://github.com/Richard5678/Machine-Learning/blob/main/heart%20problem%20-%20random%20forest.ipynb\">github</a>。"]
        }
    },
    "gradientBoosting": {
        "date": "12/01/2021",
        "en": {
            "title": "Implementing Gradient Boosting Tree from scratch in Python",
            "intro": "This blog covers the theoretical framework behind Gradient Boosting Tree and its implementation in Python without using any libraries that directly implement it.",
            "motivation": ["See previous blog on Random Forest."],
            "body": ["<h3 class=\"margin_top\">What is Gradient Boosting Tree?</h3>"
                + "Similar to Random Forest, Gradient Boosting Tree also consists of a bunch of decision trees. However, unlike Random Forest which train those trees independently, "
                + "GBT trains its trees in sequence and the next tree uses residuals of the previous tree as its target varaible instead of its label in classification tasks. "
                + "So classification tasks for GBT is actually also a regression problem since we use residuals as target variable. "
                + "By target variable we mean that when constructing the decision tree, we calculate the loss using residual instead of the label. "
                + "In this article we will present an example for classification tasks and please see a visual demonstration of how the algorithm works on <a href=\"https://www.youtube.com/watch?v=jxuNLH5dXCs&t=730s\">StatQuest</a>. "
                + "The only part that the video ommits is how it constructs the decision tree. I used the MSE loss in the example below."
                + "Basically, we use each value of all features as the split point and calculate the loss of such split using weighted MSE of sub-groups."
                + "The predicted value of each sub-group is the mean of all residuals in that group."
                + "<h3 class=\"margin_top\">Implementation</h3>"

                + "<p style=\"margin-top:30px\">The core of the program is the gbt function:</p>"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting1.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "What this function does is calling build_tree function \"treeLength\" number of times and update the progress bar accordingly. "
                + "Then the tree returned from build_tree function will be stored in a list named decision_trees which is returned to the client. "
                + "<p style=\"margin-top:30px\">Let's take a look at the function build_tree now:</p>"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting2.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "Since we are recursively building the tree up, there needs to be a base case at the start of the function. "
                + "In this case, the base case is activated when we have reached the max-depth or there is zero features. <br>"
                + "The steps we need to take to build a tree are formalized as:"
                + "<ol style=\"margin-left: 30px;\">"
                + "     <li>Find optimal split using MSE loss: (line 47 - 55)</li>"
                + "     <li>Initialize the node and fill in the decision boundary: (line 58 - 62)</li>"
                + "     <li>Construct left and right sub-trees under certain conditions: (line 66 - 69)</li>"
                + "     <li>Find the output of leaf node and update previous predictioins as well as residuals column: (line 72 - 78) </li>"
                + "</ol>"
                + "The strucutre of the build_tree function for GBT is almost identical to that of Random Forest. "
                + "The only differences are the loss we use and the updates at leaf node. "
                + "In terms of loss for finding the optimal split, MSE is used for GBT classification tasks since it is essentially a regression problem:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting3.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "The split function returns two lists, one with indices of examples less than the split point and the other with examples greater than or equal to the split point:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random3.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "Then we call the get_mse functions which returns the weighted MSE of the sub groups splitted above as well as a list containing the MSE of both groups. "
                + "The mean squared error (MSE) of a data set is give by:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting4.png\" style=\" width: 20%; height = 50%; margin: 20px\">"
                + "</div>"
                + "where y hat is the predicted value (mean residuals) and y<sub>i</sub> are all the individual residuals. For example if we have a set of 3 examples with residuals 0.1, 0 and -0.1, "
                + "then the predicted value if 0 and hence the MSE is ((0.1 - 0)<sup>2</sup> +(-0.1 - 0)<sup>2</sup> + (0 - 0)<sup>2</sup>) / 3 = 1 / 150. "
                + "Note that we are using MSE as the loss function for splitting and the less the loss the better the split. "
                + "Intuitively, we want to groups examples with similar residuals together and MSE is lower exactly when residuals are close together."
                + "<p style=\"margin-top:50px;\">After finding the optimal split point, we initialize the node and store the split point in it (line 58 - 62). </p>"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting2.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "Then we need to decide if we want to extend the tree by building up left and right sub-tree (line 66 - 69). We would only do so if both right and left sub-groups have positive lengths "
                + "and the weighted MSE of the split is less than the MSE of the current node." 
                + "Intuitively, if a sub-group contains 0 examples, then there is no point to build up the sub-tree as it will continue to find the same split to and pass 0 examples to one children and pass all examples to the other children until reaching the max-depth. "
                + "The second condition on MSE makes sure the split does indeed improving the decision tree which makes sense as we are trying to make the classifier better. <br>"
                + "<p style=\"margin-top:30px;\"></p>"
                + "The last thing we need to do find the output of the node at leaf node and update predictions as well as residuals (line 72-78). The output is calculated using the formula:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting5.png\" style=\" width: 40%; height = 50%; margin: 20px\">"
                + "</div>"
                + "where S is the set of all training examples at that node and p<sub>i</sub> is predictions made on example i from the previous tree. <br>"
                + "Then we update the predictions made on those example by:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting6.png\" style=\" width: 40%; height = 50%; margin: 20px\">"
                + "</div>"
                + "where &alpha; is the learning rate and ouput is given above. Note that we are passing the result of previous predictions multiplied by the product of learning rate and output to a sigmoid function which output values in (0, 1). "
                + "Then we update the residuals:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting7.png\" style=\" width: 40%; height = 50%; margin: 20px\">"
                + "</div>"
                + "where y<sub>i</sub> is the label of example i and p<sub>i</sub> is the new prediction made on i. <br>"
                + "<p style=\"margin-top:30px;\">To make a prediction on x, we simply pass it along with the all trained trees and its intial predictions as well as learning rate to the predict function:</p> "
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting9.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "Then the predict functions calls the classify function to find the output of each tree with x as input. Then we pass the sum of those outputs to the sigmoid function and make discrete predictions at the threshold of 0.5:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting8.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "To see how to do this concretely with a set of data, the function eval performs a n-fold cross validation on the dataset:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting10.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "Firestly, cross_val_split splits the data into n-fold equal number of sub-sets. Data of shape (m, n) will become (n_fold, m / n_fold, n): "
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random10.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "Variable set contain the all indices that have not yet been sampled. In each iteration we sample from set fold_size number of items and push them as a list into datasets. <br>"
                + "Line 54 - 56 prepares train_set and test-set. Then we pass each element in the train_set to the function predict which then calls classify to make predictions. <br>"
                + "Finally, we print the train, validation accuracy to the output as well as the mean validation accuracy at the end. "
                + "<h1 style=\"margin-top: 50px;\">Results</h1> <hr>"
                + "We are able to get 86% accuracy on the validation set with 10-fold cross validation:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting11.png\" style=\" width: 90%; height = 50%; margin: 20px\">"
                + "</div>"
                + "<h1 style=\"margin-top: 50px;\">Parameter Tuning</h1> <hr>"
                + "In Gradient Boosting Tree, there are three hyperparameters that that we need to set: max-depth, the number of trees trained and the learning rate. " 
                + "To find out the best combination of hyper parameters we include a set of possible values in the numpy array n, depth, and rate. Then we test out all possible combiantions using the sklearn library: "
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting12.png\" style=\" width: 90%; height = 50%; margin: 20px\">"
                + "</div>"
                + "The main reason to use the sklearn library is the speed is offers. Although my implementation uses numpy whenever I can to take advantage of vectorization, it is a lot of slower than sklearn. "
                + "From the following screenshot we can see that 200 trees with a max-depth of 5 and learning rate of 0.1 achive the best validation accuracy (85.94%) which is actually lower than the one I got in my implementation slightly lower than my implementation. "
                + "All three graphs shows that as max-depth, n and rate inreases, the training accuracy increases while validatioin accuracy only slightly increases when the number of trees trained increases. "
                + "Since each tree is trained based on the error made on the tree before it, it makes sence that training more trees decrease the error. I used a different set of parameters that I found using sklearn below because the sklearn implementaion did not use MSE as its loss. "
                + "However, my implementation achived comparable performance with a different set of hyperparameters."
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting13.png\" style=\" width: 90%; height = 50%; margin: 20px\">"
                + "</div>"
                + "<h1 style=\"margin-top: 50px;\">Implementation</h1>"
                + "<hr>"
                + "See <a href=\"https://github.com/Richard5678/Machine-Learning/blob/main/heart%20problem%20-%20gradient%20boosted%20tree.ipynb\">github</a> for details. "
            ]
        },
        "zh": {
            "title": "用Python从零来开始软件实现梯度提升树",
            "intro": "这篇博客概述了梯度提升树的理论框架以及其Python的软件实现(没有运用任何已有的直接实现其的函式库)。",
            "motivation": ["见前一篇关于随机森林的博客。"],

            "body": ["<h3 class=\"margin_top\">什么是梯度提升树?</h3>"
                + "和随机森林相似，梯度提升树也是有很多决策树组成的。然而，和随机森林不一样的是这些决策树是是按照顺序一一训练的(每个树用前一个树的残差作为其的目标变量而不是其标签)而不是独立训练。"
                + "所以梯度提升树的分类问题实际是也是一个回归问题（残差作为目标变量）。"
                + "目标变量的意思是在建造决策树时，我们用残差而不是标签来计算成分函数值。"
                + "在这篇文章中会呈现一个分类的案例。如果想要通过动画演示了解这个算法是怎么工作的请到<a href=\"https://www.youtube.com/watch?v=jxuNLH5dXCs&t=730s\">StatQuest</a>. "
                + "视频中唯一漏掉的部分就是如果建造决策树。我在以下案例中使用了均方差作为成分函数。"
                + "简单来说，我用了每个特征的所以数值作为分界点并计算了分流后的加权均方差。"
                + "每个分流后小组的预测值为其所有训练例子的残差的平均数。"

                + "<h3 class='margin_top'>软件实现</h3>"
                + "首先，我们正式给出随机森林训练一个决策树的步骤："
                + "<ol style=\"margin-left:10%\">"
                + "     <li>随机抽取特征及训练例子</li>"
                + "     <li>通过计算每个抽取特征的所有数值成本来找到最佳分界点</li>"
                + "     <li>将现在节点的训练例子根据以上找到的分界点分成两组</li>"
                + "     <li>递归的搭建子决策树直至到设定的最深深度</li>"
                + "</ol>"
                + "<p style=\"margin-top:30px\">整个算法的核心为以下的gbt函数:</p>"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting1.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "这个函数会重复使用build_tree函数\"treeLength\"遍并更新进度条。"
                + "之后从build_tree返还的树会被保存在decision_trees里并在最后返回给用户（client)。"
                + "<p style=\"margin-top:30px\">以下为build_tree 函数:</p>"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting2.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "因为我们是运用递归来建造书的，所以我们需要一个终止条件（base case)。"
                + "在这个案例中终止条件会在已到最高深度或有0个特征时激活。"
                + "之后这个函数可以被分为4大部分："
                + "<ol style=\"margin-left:10%\">"
                + "     <li>找到最佳分界点: 47 - 55行</li>"
                + "     <li>建造节点并添加节点信息: 58 - 62行</li>"
                + "     <li>在一定情况下建造出左右子树: 66 - 69行</li>"
                + "     <li>在叶节点找到节点的标签: 72 - 78行</li>"    
                + "</ol>"
                + "GBT的build_tree函数的结构和随机森林的build_tree几乎完全一样。仅有的两个区别是这个我们用了不同的成本函数，还有就是在叶节点时我们需要更新预测值和残差。"
                + "在寻找最佳分界点时，我的梯度提升树运用了均方差因为其实际上是个回归问题:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting3.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "split函数返还两个list, 一个包含所有小于分界点的训练例子的索引，另一个包含所有大于等于分界点训练例子的索引:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random3.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "之后我们使用get_mse函数得到子群的加权均方差以及每个子群的均方差。均方差的公式为:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting4.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "y帽为预测的数值(残差平均数)而y<sub>i</sub>为每个数据的残差。加入我们有三个例子(残差分别为0.1, 0和-0.1)，那么其均方差为((0.1 - 0)<sup>2</sup> + (-0.1 - 0)<sup>2</sup> + (0 - 0)<sup>2</sup>) / 3 = 1 / 150。"
                + "需要注意的是我们用均方差作为每次分流的成本函数所以均方差越小，那么分流就越好。直观上来讲，我们希望每个小组的数据的残差更靠近彼此而均方差正是在目标变量靠近彼此时更小。"
                + "在找到最佳分界点后，我们将节点初始化并加入关于分界点的信息(58 - 62行)。"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting2.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "之后我们需要决定我们是否想要通过搭建所有子树来延展整个决策树(66 - 69行)。我们只在所有左右两个子群都有至少一个例子"
                + "并且它们的加权基尼系数平均数小于现在的节点的基尼系数时才继续大家子树。" 
                + "直观上，如果一个子群有0个训练例子，那么没有在继续搭建子树的必要因为其子数会找到一样的分界点并不停的向一侧延展决策树知道遇到最高深度。"
                + "第二个关于均方差的条件是用来确保我们的决策树在分流后确实比分流前要更好。<br>"
                + "<p style=\"margin-top:30px;\"></p>"
                + "最后我们要做的是找到叶节点的输出(72-78行)。输出的数值使用以下公式计算的:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting5.png\" style=\" width: 40%; height = 50%; margin: 20px\">"
                + "</div>"
                + "S为在这个节点所有的训练例子, p<sub>i</sub>为前一个树对第i个例子做出的预测。之后我们可以更新预测为:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting6.png\" style=\" width: 40%; height = 50%; margin: 20px\">"
                + "</div>"
                + "&alpha;为学习进度，output为上面计算的输出。需要注意的是我们实际上将之前预测与学习进度输出的积的和送进了sigmoid函数(其输出值在(0, 1))。之后我们更新残差:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting7.png\" style=\" width: 40%; height = 50%; margin: 20px\">"
                + "</div>"
                + "y<sub>i</sub>为第i和例子的标签，p<sub>i</sub>为第i和例子的预测。"
                + "<span style=\"margin-top: 30px;\"></span>当预测x时, 我们只需将其与所有训练好的树及其最初的预测和学习进度送到predict函数中:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting8.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "之后predict函数运用classify函数来的到每个树在输入x后的输出。之后我们将所有输出的和送到sigmoid函数中并将其根据0.5为分界线分类："
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting9.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "我们可以通过以下的eval函数来看看如何通过n折交叉验证（n-fold cross-validation)来软件实现梯度提升树的预测:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting10.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "首先, cross_val_split函数将数据分为n-fold个等等大小的部分。(m, n)形状的数据会变为(n_fold, m / n_fold, n): "
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random10.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "set变量包含了所有没被抽到过的索引。在每轮循环中我们从set中抽取fold_size大小的数据并将他们存到datasets里。<br>"
                + "40 - 42行准备了train_set和test-set。之后我们将train_set每个训练例子送到predict函数，其将使用classify函数来给出最终的预测:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/random11.png\" style=\" width: 70%; height = 50%; margin: 20px\">"
                + "</div>"
                + "最后，我们打出训练以及验证正确率并在最后打出平均验证正确率。"
                + "<h1 style=\"margin-top: 50px;\">结果</h1> <hr>"
                + "我们能够10折交叉验证的验证数据中得到86%的平均正确率:"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting11.png\" style=\" width: 90%; height = 50%; margin: 20px\">"
                + "</div>"
                + "<h1 style=\"margin-top: 50px;\">参数调试</h1> <hr>"
                + "在梯度提升树中我们有三个超参数需要设置：最高深度，训练的决策树数量以及学习进度。" 
                + "为了找到最佳的超参数组合，我用了三个numpy array来存了一些可能的数值。之后我们用sklearn函式库来测试所有的组合: "
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting12.png\" style=\" width: 90%; height = 50%; margin: 20px\">"
                + "</div>"
                + "运用sklearn函式库的主要原因是因为其的速度。尽管我在我写的程序中尽可能的运用numpy来利用其向量化带来的速度提升，但是还是比sklearn慢很多。"
                + "在以下的输出中我们可能看到200个树，最高深度5以及学习进度0.1的组合得到了最高的验证正确率(85.94%)。这个正确率实际上比我的版本的到的还要低一点。"
                + "以下三个图像都能看出当最高深度，训练树的数量和学习进度提高时，训练正确率会有所提高，然而验证正确率除了训练树的数量以外并没有看到显著提高。"
                + "由于每个树是根据前一个树的残差最为目标变量训练的，训练的树越多，误差确实会下降。我在训练时用了与以下不一样的一组超参数组合因为sklearn函式库没有用均方差作为成本函数。"
                + "然而我的软件实现和sklearn的版本得到了相似的正确率。"
                + "<div style=\"text-align:center;\">"
                + "     <img src=\"/static/home/gradientBoosting13.png\" style=\" width: 90%; height = 50%; margin: 20px\">"
                + "</div>"
                + "<h1 style=\"margin-top: 50px;\">软件实现</h1>"
                + "<hr>"
                + "详情见<a href=\"https://github.com/Richard5678/Machine-Learning/blob/main/heart%20problem%20-%20gradient%20boosted%20tree.ipynb\">github</a>。"]
        }
    }
}